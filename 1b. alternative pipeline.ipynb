{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf7450a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformer_srl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformer_srl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m predictors\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# ── deterministic seeds ─────────────────────────────────────────\u001b[39;00m\n\u001b[0;32m     26\u001b[0m SEED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformer_srl'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "# SRL → Tuple Extraction Pipeline (Transformer‑SRL + spaCy)\n",
    "\n",
    "Multi‑cell notebook to generate subject–predicate–object tuples, event IDs, and\n",
    "VSA‑friendly nodes/edges from a sentence. Uses `transformer-srl` in place of\n",
    "AllenNLP. Each cell prints intermediate structures for rapid diagnosis.\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Install once per environment (GPU optional)\n",
    "# Note: uncomment next two lines if running in a fresh kernel.\n",
    "# !pip install -q transformer-srl spacy==3.7.2 torch jsonschema\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# %%\n",
    "import json, re, uuid, hashlib\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import random, numpy as np, torch\n",
    "import spacy\n",
    "from transformer_srl import predictors\n",
    "\n",
    "# ── deterministic seeds ─────────────────────────────────────────\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "print(\"Seeds set →\", SEED)\n",
    "\n",
    "# %%\n",
    "# ── load NLP models once ───────────────────────────────────────\n",
    "NLP = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\", \"lemmatizer\"])\n",
    "print(\"spaCy model loaded → en_core_web_sm\")\n",
    "\n",
    "MODEL_PATH = \"srl_bert_base_conll2012.tar.gz\"  # download once and cache\n",
    "if not Path(MODEL_PATH).exists():\n",
    "    import urllib.request, tempfile\n",
    "    print(\"Downloading SRL model (≈420 MB)…\")\n",
    "    url = (\n",
    "        \"https://storage.googleapis.com/allennlp-public-models/bert-base-srl-2020.12.15.tar.gz\"\n",
    "    )\n",
    "    urllib.request.urlretrieve(url, MODEL_PATH)\n",
    "    print(\"✓ Downloaded →\", MODEL_PATH)\n",
    "\n",
    "SRL = predictors.SrlTransformersPredictor.from_path(MODEL_PATH)\n",
    "print(\"Transformer‑SRL predictor ready →\", SRL)\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 1. Input sentence\n",
    "Modify `SENTENCE` below and re‑run from here to inspect pipeline outputs.\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "SENTENCE = \"I saw a white dog chase the brown cat quickly in the backyard.\"\n",
    "print(\"Sentence →\", SENTENCE)\n",
    "\n",
    "# %%\n",
    "# ── spaCy tokenisation + dependencies ──────────────────────────\n",
    "doc = NLP(SENTENCE)\n",
    "\n",
    "TOKENS = [t.text for t in doc]\n",
    "DEPS = [(t.text, t.dep_, t.head.text, t.i, t.head.i) for t in doc]\n",
    "\n",
    "print(\"Tokens (spaCy):\", TOKENS)\n",
    "print(\"Dependency triples (token, dep, head):\")\n",
    "for tok, dep, head, *_ in DEPS:\n",
    "    print(f\"  {tok:>10} ─{dep:<8}→ {head}\")\n",
    "\n",
    "# %%\n",
    "# ── Transformer‑SRL frames ─────────────────────────────────────\n",
    "srl_out = SRL.predict(sentence=SENTENCE)\n",
    "FRAMES = srl_out[\"verbs\"]  # list of dicts: {verb, tags}\n",
    "\n",
    "print(\"SRL Frames (predicate → role tags):\")\n",
    "for f in FRAMES:\n",
    "    print(f\"{f['verb']:<8} → {f['description']}\")\n",
    "\n",
    "# %%\n",
    "# ── Role dictionaries ──────────────────────────────────────────\n",
    "DEP2ROLE = {\n",
    "    \"nsubj\": \"Subject\", \"csubj\": \"Subject\",\n",
    "    \"dobj\": \"Object\", \"obj\": \"Object\",\n",
    "    \"iobj\": \"IndirectObject\",\n",
    "    \"advmod\": \"Attr\", \"amod\": \"Attr\",\n",
    "}\n",
    "SRL2ROLE = {\n",
    "    \"ARG0\": \"Subject\", \"ARG1\": \"Object\", \"ARG2\": \"IndirectObject\",\n",
    "    \"AM-TMP\": \"Time\", \"AM-LOC\": \"IndirectObject\", \"AM-MNR\": \"Attr\",\n",
    "}\n",
    "\n",
    "# Build proto‑tuples from dependencies + SRL\n",
    "proto_tuples = []\n",
    "for tok, dep, head, ti, hi in DEPS:\n",
    "    if dep in DEP2ROLE:\n",
    "        proto_tuples.append({\"role\": DEP2ROLE[dep], \"span\": (ti, ti+1), \"text\": tok})\n",
    "\n",
    "for vf in FRAMES:\n",
    "    tags = vf[\"tags\"]\n",
    "    for idx, tag in enumerate(tags):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            role_tag = tag[2:]\n",
    "            end = idx\n",
    "            while end+1 < len(tags) and tags[end+1].startswith(\"I-\"):\n",
    "                end += 1\n",
    "            if role_tag in SRL2ROLE:\n",
    "                span_text = \" \".join(TOKENS[idx:end+1])\n",
    "                proto_tuples.append({\"role\": SRL2ROLE[role_tag],\n",
    "                                     \"span\": (idx, end+1),\n",
    "                                     \"text\": span_text})\n",
    "\n",
    "print(\"Proto‑tuples →\", len(proto_tuples))\n",
    "for t in proto_tuples:\n",
    "    print(t)\n",
    "\n",
    "# %%\n",
    "# ── Event detection: finite verbs as event anchors ─────────────\n",
    "eids, eid_counter = {}, 0\n",
    "for tok in doc:\n",
    "    if tok.pos_ == \"VERB\" and tok.morph.get(\"VerbForm\") != [\"Inf\"]:\n",
    "        eid_counter += 1\n",
    "        eids[tok.i] = f\"e{eid_counter}\"\n",
    "print(\"EIDs detected →\", eids)\n",
    "\n",
    "# Attach eid to tuples (nearest governing verb)\n",
    "for t in proto_tuples:\n",
    "    head_i = doc[t[\"span\"][0]].head.i\n",
    "    t[\"eid\"] = eids.get(head_i, \"e1\")\n",
    "\n",
    "# %%\n",
    "# ── Node construction (CHV schema) ─────────────────────────────\n",
    "\n",
    "def slug(text):\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", text.lower())[:32] or \"unk\"\n",
    "\n",
    "nodes, node_index = [], {}\n",
    "for tup in proto_tuples:\n",
    "    base = f\"spo:{slug(tup['text'])}@{tup['eid']}\"\n",
    "    if base not in node_index:\n",
    "        node_index[base] = base\n",
    "        nodes.append({\n",
    "            \"id\": base,\n",
    "            \"filler\": tup[\"text\"],\n",
    "            \"roles\": [tup[\"role\"]],\n",
    "            \"eid_set\": [tup[\"eid\"]],\n",
    "            \"ntype\": \"spo\",\n",
    "        })\n",
    "    else:\n",
    "        for n in nodes:\n",
    "            if n[\"id\"] == base and tup[\"role\"] not in n[\"roles\"]:\n",
    "                n[\"roles\"].append(tup[\"role\"])\n",
    "\n",
    "# explicit predicate nodes\n",
    "action_nodes = []\n",
    "for tok_i, eid in eids.items():\n",
    "    pred = doc[tok_i].lemma_\n",
    "    n_id = f\"spo:{slug(pred)}@{eid}\"\n",
    "    if n_id not in node_index:\n",
    "        node_index[n_id] = n_id\n",
    "        action_nodes.append({\n",
    "            \"id\": n_id,\n",
    "            \"filler\": pred,\n",
    "            \"roles\": [\"Predicate\"],\n",
    "            \"eid_set\": [eid],\n",
    "            \"ntype\": \"spo\",\n",
    "        })\n",
    "\n",
    "nodes.extend(action_nodes)\n",
    "\n",
    "# event stubs\n",
    "for eid in set(eids.values()):\n",
    "    nodes.append({\"id\": f\"evt:{eid}\", \"filler\": eid,\n",
    "                  \"roles\": [\"Event\"], \"eid_set\": [eid], \"ntype\": \"event\"})\n",
    "\n",
    "# single CHV hub\n",
    "nodes.append({\"id\": \"chv:main\", \"filler\": \"CHV\",\n",
    "              \"roles\": [\"CHV\"], \"eid_set\": [], \"ntype\": \"chv\"})\n",
    "\n",
    "print(\"Total nodes →\", len(nodes))\n",
    "\n",
    "# %%\n",
    "# ── Edge construction ─────────────────────────────────────────\n",
    "edges = []\n",
    "\n",
    "def add_edge(src, tgt, kind):\n",
    "    edges.append({\"source\": src, \"target\": tgt, \"kind\": kind})\n",
    "\n",
    "for tup in proto_tuples:\n",
    "    head_token = doc[tup[\"span\"][0]].head\n",
    "    pred_id = f\"spo:{slug(head_token.lemma_)}@{tup['eid']}\"\n",
    "    if tup[\"role\"] == \"Subject\":\n",
    "        add_edge(f\"spo:{slug(tup['text'])}@{tup['eid']}\", pred_id, \"S-P\")\n",
    "    elif tup[\"role\"] == \"Object\":\n",
    "        add_edge(pred_id, f\"spo:{slug(tup['text'])}@{tup['eid']}\", \"P-O\")\n",
    "\n",
    "for eid in set(eids.values()):\n",
    "    subj_nodes = [n for n in nodes if \"Subject\" in n[\"roles\"] and eid in n[\"eid_set\"]]\n",
    "    if subj_nodes:\n",
    "        add_edge(f\"evt:{eid}\", subj_nodes[0][\"id\"], \"event-pred\")\n",
    "\n",
    "outer_eid = min(set(eids.values()))\n",
    "last_obj_nodes = [n[\"id\"] for n in nodes if \"Object\" in n[\"roles\"] and outer_eid in n[\"eid_set\"]]\n",
    "if last_obj_nodes:\n",
    "    add_edge(last_obj_nodes[-1], \"chv:main\", \"binder\")\n",
    "\n",
    "print(\"Total edges →\", len(edges))\n",
    "\n",
    "# %%\n",
    "# ── Hull construction (visual grouping) ───────────────────────\n",
    "hulls = []\n",
    "for eid in set(eids.values()):\n",
    "    members = [n[\"id\"] for n in nodes if eid in n[\"eid_set\"] and n[\"ntype\"] == \"spo\"]\n",
    "    if members:\n",
    "        hulls.append({\"eid\": eid, \"members\": members})\n",
    "print(\"Hulls →\", hulls)\n",
    "\n",
    "# %%\n",
    "# ── Payload assembly & JSON Schema check ─────────────────────\n",
    "\n",
    "payload = {\n",
    "    \"version\": \"2.1\",\n",
    "    \"sentence\": SENTENCE,\n",
    "    \"nodes\": nodes,\n",
    "    \"edges\": edges,\n",
    "    \"layouts\": {\"hulls\": hulls},\n",
    "}\n",
    "\n",
    "print(json.dumps(payload, indent=2, ensure_ascii=False))\n",
    "\n",
    "SCHEMA_PATH = \"schema.json\"\n",
    "if Path(SCHEMA_PATH).exists():\n",
    "    from jsonschema import Draft202012Validator\n",
    "    SCHEMA = json.load(open(SCHEMA_PATH))\n",
    "    Draft202012Validator.check_schema(SCHEMA)\n",
    "    Draft202012Validator(SCHEMA).validate(payload)\n",
    "    print(\"✅  Payload is schema‑valid\")\n",
    "else:\n",
    "    print(\"⚠️  schema.json not found ‑‑ skipping validation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e8531",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
